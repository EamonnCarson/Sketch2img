{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import PIL\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants and helper methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Make sure that you run all the cells in this section!</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '../learning/datasets/sketchy/256x256'\n",
    "PHOTOS_AUG = 'tx_000100000000'\n",
    "PHOTOS_DIR = os.path.join(DATA_DIR, 'photo', PHOTOS_AUG)\n",
    "SKETCHES_AUG = 'tx_000000000010'\n",
    "SKETCHES_DIR = os.path.join(DATA_DIR, 'sketch', SKETCHES_AUG)\n",
    "INFO_DIR = '../learning/datasets/info-06-04/info'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create some helpful helper methods to convert between class labels, ImageNet IDs, and class ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_imagenet_id_class_map(photos_dir):\n",
    "    \"\"\"Returns a dictionary that maps ImageNet ID to class label, and vice versa.\"\"\"\n",
    "    d = {}\n",
    "    for folder in os.listdir(photos_dir):\n",
    "        file = os.listdir(os.path.join(photos_dir, folder))[0]\n",
    "        imagenet_id = file.split('_')[0]\n",
    "        d[imagenet_id] = folder\n",
    "        d[folder] = imagenet_id\n",
    "    return d\n",
    "\n",
    "imagenet_id_class_map = generate_imagenet_id_class_map(PHOTOS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_labels_id_map(photos_dir):\n",
    "    \"\"\"Returns a list of class labels\"\"\"\n",
    "    d = {}\n",
    "    for i, label in enumerate(sorted(os.listdir(photos_dir))):\n",
    "        d[i] = label\n",
    "        d[label] = i\n",
    "    return d\n",
    "\n",
    "labels_id_map = generate_labels_id_map(PHOTOS_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we need to filter out some sketches, we'll create a special dataset for them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SketchesDataset(Dataset):\n",
    "    \"\"\"A custom Dataset class for sketches. \"\"\"\n",
    "    \n",
    "    def __init__(self, sketches_dir, info_dir, transform=None, remove_error=True, remove_ambiguous=False, \n",
    "                 remove_pose=False, remove_context=False):\n",
    "        \"\"\"\n",
    "        Initialize the sketches dataset.\n",
    "        \n",
    "        Args:\n",
    "            sketches_dir (str): directory of sketches, divided by class\n",
    "            info_dir (str): directory with additional information about the sketches\n",
    "            remove_error (bool): set to True to remove sketches classified as erroneous\n",
    "            remove_ambiguous (bool): set to True to remove sketches classified as ambiguous\n",
    "            remove_pose (bool): set to True to remove sketches drawn from a wrong pose/perspective\n",
    "            remove_context (bool): set to True to remove sketches with extraneous details\n",
    "        \"\"\"\n",
    "        self.sketches_dir = sketches_dir\n",
    "        self.info_dir = info_dir\n",
    "        self.transform = transform\n",
    "        self.invalid = [line for line in open(os.path.join(info_dir, 'invalid-error.txt'), 'r')]\n",
    "        self.stats = pd.read_csv(os.path.join(info_dir, 'stats.csv'))\n",
    "        if remove_error:\n",
    "            self.stats = self.stats.loc[self.stats['Error?'] == 0]\n",
    "        if remove_ambiguous:\n",
    "            self.stats = self.stats.loc[self.stats['Ambiguous?'] == 0]\n",
    "        if remove_pose:\n",
    "            self.stats = self.stats.loc[self.stats['WrongPose?'] == 0]\n",
    "        if remove_context:\n",
    "            self.stats = self.stats.loc[self.stats['Context?'] == 0]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.stats)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.stats.iloc[idx]\n",
    "        class_folder = row['Category'].replace(' ', '_')\n",
    "        sketch_file = f\"{row['ImageNetID']}-{row['SketchID']}.png\"\n",
    "        sketch_path = os.path.join(self.sketches_dir, class_folder, sketch_file)\n",
    "        with open(sketch_path, 'rb') as f:\n",
    "            image = PIL.Image.open(f).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, labels_id_map[class_folder]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to invert the colors of the sketches, so the background is black and the sketch lines are white. We'll write a custom `Transform` for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvertTransform:\n",
    "    \"\"\"A transform that takes a Tensor with values in [0, 1], and inverts those values.\"\"\"\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        return 1 - sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You don't need to run the cells in this section if you have dataset_stats.npz or you change `SketchesDataset`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have two methods below which calculate the mean and standard deviation of a dataset, which allow us to normalize that dataset later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_mean(dataset, batch_size=100):\n",
    "    \"\"\"Computes the mean of the dataset.\"\"\"\n",
    "    dl = DataLoader(dataset, batch_size=batch_size)\n",
    "    mean = 0.\n",
    "    for batch, _ in dl:\n",
    "        batch_samples = batch.size(0)  # batch size (the last batch can have smaller size!)\n",
    "        batch = batch.view(batch_samples, batch.size(1), -1)\n",
    "        mean += batch.mean(2).sum(0)\n",
    "    mean /= len(dl.dataset)\n",
    "    return mean\n",
    "\n",
    "def dataset_std(dataset, mean, batch_size=100):\n",
    "    \"\"\"Computes the standard deviation of the dataset.\"\"\"\n",
    "    c, h, w = dataset[0][0].size()\n",
    "    dl = DataLoader(dataset, batch_size=batch_size)\n",
    "    var = 0.\n",
    "    for batch, _ in dl:\n",
    "        batch_samples = batch.size(0)  # batch size (the last batch can have smaller size!)\n",
    "        batch = batch.view(batch_samples, c, -1)\n",
    "        var += ((batch - mean.unsqueeze(1))**2).sum([0, 2])\n",
    "    std = torch.sqrt(var / (len(dl.dataset) * h * w))\n",
    "    return std\n",
    "\n",
    "def dataset_scaling(dataset, batch_size=100):\n",
    "    \"\"\"\n",
    "    Computes how much to scale dataset to be in range [-1, 1] after mean subtraction\n",
    "    Assumes dataset passed in has already been through mean subtraction\n",
    "    \"\"\"\n",
    "    dl = DataLoader(dataset, batch_size=batch_size)\n",
    "    min_value = 1\n",
    "    max_value = -1\n",
    "    for batch, _ in dl:\n",
    "        batch_samples = batch.size(0)  # batch size (the last batch can have smaller size!)\n",
    "        batch = batch.view(batch_samples, batch.size(1), -1)\n",
    "        max_value = max(max_value, torch.max(batch).item())\n",
    "        min_value = min(min_value, torch.min(batch).item())\n",
    "    scaling = max(abs(min_value), abs(max_value))\n",
    "    return scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Photos dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For photos, we can simply use PyTorch's `ImageFolder`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we calculate the mean and standard deviation of the photos dataset, so we can normalize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time for mean: 26.669545827026013\n",
      "tensor([0.4714, 0.4475, 0.3958])\n"
     ]
    }
   ],
   "source": [
    "photos_dataset = ImageFolder(root=PHOTOS_DIR, transform=transforms.ToTensor())\n",
    "start = timer()\n",
    "photos_mean = dataset_mean(photos_dataset)\n",
    "print(f\"time for mean: {timer() - start}\")\n",
    "print(photos_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time for std: 31.831889882974792\n",
      "tensor([0.2679, 0.2565, 0.2746])\n"
     ]
    }
   ],
   "source": [
    "start = timer()\n",
    "photos_std = dataset_std(photos_dataset, photos_mean)\n",
    "print(f\"time for std: {timer() - start}\")\n",
    "print(photos_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to find how much to scale the dataset after mean subtraction to make it in the range [-1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time for scaling: 30.865477380983066\n",
      "0.6042009592056274\n"
     ]
    }
   ],
   "source": [
    "photos_dataset = ImageFolder(root=PHOTOS_DIR, transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=photos_mean, std=np.array([1, 1, 1]))\n",
    "]))\n",
    "start = timer()\n",
    "photos_scaling = dataset_scaling(photos_dataset)\n",
    "print(f\"time for scaling: {timer() - start}\")\n",
    "print(photos_scaling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sketches dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have already created the cells for the sketches dataset in [Constants and helper methods](#Constants-and-helper-methods)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also need to find the mean and std of the sketches dataset to normalize, just like the photos dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time for mean: 122.28621749696322\n",
      "tensor([0.0388, 0.0388, 0.0388])\n"
     ]
    }
   ],
   "source": [
    "sketches_dataset = SketchesDataset(SKETCHES_DIR, INFO_DIR, transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    InvertTransform()\n",
    "]))\n",
    "start = timer()\n",
    "sketches_mean = dataset_mean(sketches_dataset)\n",
    "print(f\"time for mean: {timer() - start}\")\n",
    "print(sketches_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time for std: 146.3518540650257\n",
      "tensor([0.1892, 0.1892, 0.1892])\n"
     ]
    }
   ],
   "source": [
    "start = timer()\n",
    "sketches_std = dataset_std(sketches_dataset, sketches_mean)\n",
    "print(f\"time for std: {timer() - start}\")\n",
    "print(sketches_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to find how much to scale the dataset after mean subtraction to make it in the range [-1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time for scaling: 139.5827461790177\n",
      "0.9611556529998779\n"
     ]
    }
   ],
   "source": [
    "sketches_dataset = SketchesDataset(SKETCHES_DIR, INFO_DIR, transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    InvertTransform(),\n",
    "    transforms.Normalize(mean=sketches_mean, std=np.array([1, 1, 1]))\n",
    "]))\n",
    "start = timer()\n",
    "sketches_scaling = dataset_scaling(sketches_dataset)\n",
    "print(f\"time for scaling: {timer() - start}\")\n",
    "print(sketches_scaling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can save the means and standard deviations of the datasets into a file, so we don't have to run this code again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(\"dataset_stats\", photos_mean=photos_mean.numpy(), photos_std=photos_std.numpy(), photos_scaling=photos_scaling,\n",
    "        sketches_mean=sketches_mean.numpy(), sketches_std=sketches_std.numpy(), sketches_scaling=sketches_scaling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code below, assuming that you have already run the code from the section [Pre-processing images](#Pre-processing-images) or have the dataset_stats.npz file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.47139016 0.44750962 0.395799  ] [0.2678542 0.2564591 0.274611 ] 0.6042009592056274\n",
      "[0.03884432 0.03884432 0.03884432] [0.18919694 0.18919694 0.18919694] 0.9611556529998779\n"
     ]
    }
   ],
   "source": [
    "npzfile = np.load(\"dataset_stats.npz\")\n",
    "photos_mean = npzfile['photos_mean']\n",
    "photos_std = npzfile['photos_std']\n",
    "photos_scaling = npzfile['photos_scaling']\n",
    "sketches_mean = npzfile['sketches_mean']\n",
    "sketches_std = npzfile['sketches_std']\n",
    "sketches_scaling = npzfile['sketches_scaling']\n",
    "print(photos_mean, photos_std, photos_scaling)\n",
    "print(sketches_mean, sketches_std, sketches_scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "photos_dataset = ImageFolder(root=PHOTOS_DIR, transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=photos_mean, std=np.array([photos_scaling, photos_scaling, photos_scaling]))\n",
    "]))\n",
    "photos_dl = DataLoader(photos_dataset, batch_size=50, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sketches_dataset = SketchesDataset(SKETCHES_DIR, INFO_DIR, transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    InvertTransform(),\n",
    "    transforms.Normalize(mean=sketches_mean, std=np.array([sketches_scaling, sketches_scaling, sketches_scaling]))\n",
    "]))\n",
    "sketches_dl = DataLoader(sketches_dataset, batch_size=50, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from generator import Generator\n",
    "from discriminator import Discriminator\n",
    "from dataset import load_sketchygan_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): MRU(\n",
      "    (conv_mi): ModuleList(\n",
      "      (0): Conv2d(6, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): Sigmoid()\n",
      "    )\n",
      "    (conv_ni): ModuleList(\n",
      "      (0): Conv2d(6, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): Sigmoid()\n",
      "    )\n",
      "    (conv_zi): ModuleList(\n",
      "      (0): Conv2d(6, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.01)\n",
      "    )\n",
      "    (conv_xi): ModuleList(\n",
      "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.01)\n",
      "    )\n",
      "  )\n",
      "  (1): Conv2d(64, 64, kernel_size=(2, 2), stride=(2, 2))\n",
      ")\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "forward() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-0af68a8bf96c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0md\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/git/Sketch2img/exploration_david/discriminator.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinear_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mdis_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc_dis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/git/Sketch2img/exploration_david/generator.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_maps, images)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \"\"\"\n\u001b[1;32m     72\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0mlayer1_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_maps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_pool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mlayer2_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer1_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "ds, dl = load_sketchygan_dataset(8)\n",
    "d = Discriminator(125, 3)\n",
    "image = ds[0][0]\n",
    "image = image[:, :64, :64]\n",
    "image = image.view(1, 3, 64, 64)\n",
    "d(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from discriminator import Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (encoder): Encoder(\n",
       "    (image_pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    (layer1): ModuleList(\n",
       "      (0): MRU(\n",
       "        (conv_mi): ModuleList(\n",
       "          (0): Conv2d(9, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "        (conv_ni): ModuleList(\n",
       "          (0): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "        (conv_zi): ModuleList(\n",
       "          (0): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): LeakyReLU(negative_slope=0.01)\n",
       "        )\n",
       "        (conv_xi): ModuleList(\n",
       "          (0): Conv2d(6, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): LeakyReLU(negative_slope=0.01)\n",
       "        )\n",
       "      )\n",
       "      (1): Conv2d(64, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "    )\n",
       "    (layer2): ModuleList(\n",
       "      (0): MRU(\n",
       "        (conv_mi): ModuleList(\n",
       "          (0): Conv2d(67, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "        (conv_ni): ModuleList(\n",
       "          (0): Conv2d(67, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "        (conv_zi): ModuleList(\n",
       "          (0): Conv2d(67, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): LeakyReLU(negative_slope=0.01)\n",
       "        )\n",
       "        (conv_xi): ModuleList(\n",
       "          (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): LeakyReLU(negative_slope=0.01)\n",
       "        )\n",
       "      )\n",
       "      (1): Conv2d(128, 128, kernel_size=(2, 2), stride=(2, 2))\n",
       "    )\n",
       "    (layer3): ModuleList(\n",
       "      (0): MRU(\n",
       "        (conv_mi): ModuleList(\n",
       "          (0): Conv2d(131, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "        (conv_ni): ModuleList(\n",
       "          (0): Conv2d(131, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "        (conv_zi): ModuleList(\n",
       "          (0): Conv2d(131, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): LeakyReLU(negative_slope=0.01)\n",
       "        )\n",
       "        (conv_xi): ModuleList(\n",
       "          (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): LeakyReLU(negative_slope=0.01)\n",
       "        )\n",
       "      )\n",
       "      (1): Conv2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "    )\n",
       "    (layer4): ModuleList(\n",
       "      (0): MRU(\n",
       "        (conv_mi): ModuleList(\n",
       "          (0): Conv2d(259, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "        (conv_ni): ModuleList(\n",
       "          (0): Conv2d(259, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "        (conv_zi): ModuleList(\n",
       "          (0): Conv2d(259, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): LeakyReLU(negative_slope=0.01)\n",
       "        )\n",
       "        (conv_xi): ModuleList(\n",
       "          (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): LeakyReLU(negative_slope=0.01)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc_dis): Linear(in_features=32768, out_features=1, bias=True)\n",
       "  (fc_aux): Linear(in_features=32768, out_features=125, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Discriminator(125, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
